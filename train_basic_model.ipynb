{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d93bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7cd4d2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from utils import get_test_data, visualize_graph, get_stub_r\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import awkward as ak\n",
    "import copy\n",
    "\n",
    "import os\n",
    "\n",
    "import h5py\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "import numpy as np\n",
    "from pyntcloud import PyntCloud\n",
    "\n",
    "# viz\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.utils.convert import from_networkx, to_networkx\n",
    "\n",
    "# Do we need normalization?\n",
    "def resize_and_format_data(points, image):\n",
    "    pass\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "#torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# Get data\n",
    "branches = get_test_data('pd')\n",
    "print(branches.head())\n",
    "#br=get_test_data('pd')\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#print(br.head())\n",
    "\n",
    "# Keep this\n",
    "#branches = get_test_data('ak')\n",
    "#generate_hdf5_dataset_with_padding(branches, 'data/point_clouds.hd5')\n",
    "#dataset = get_training_dataset('data/point_clouds.hd5')\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_point_cloud(arr):\n",
    "\n",
    "    arr['stubR'] = get_stub_r(arr['stubType'], arr['stubDetId'], arr['stubEta'], arr['stubLogicLayer'])\n",
    "    for index, row in arr.iterrows():\n",
    "        if not ( len(arr['stubR'][index])==len(arr['stubEta'][index]) and len(arr['stubEta'][index])==len(arr['stubPhi'][index])):\n",
    "            print('HUGE PROBLEM, sizes not match: R,eta,phi ', len(arr['stubR'][index]), len(arr['stubEta'][index]), len(arr['stubPhi'][index]))\n",
    "    arr=arr.rename(columns={\"stubR\": \"x\", \"stubEta\": \"y\", \"stubPhi\": \"z\"}, errors=\"raise\")\n",
    "\n",
    "    keep=['x', 'y', 'z', 'stubProc', 'stubPhiB', 'stubEtaSigma', 'stubQuality', 'stubBx', 'stubDetId', 'stubType', 'stubTiming', 'stubLogicLayer', 'muonPt']\n",
    "\n",
    "    sky=[]\n",
    "    for index, row in arr.iterrows():\n",
    "        # Here now I need to enrich this with the stublogiclayer etc, for the edges\n",
    "        pc = {} #{'x': row['stubR'], 'y': row['stubEta'], 'z': row['stubPhi'], 'stubLogicLayer': row['stubLogicLayer']} # down the line maybe convert to actual cartesian coordinates\n",
    "        for label in keep:\n",
    "            pc[label] = row[label]\n",
    "        # Create a PyntCloud object from the DataFrame\n",
    "        if len(pc['x'])==0:\n",
    "            continue\n",
    "        cloud = PyntCloud(pd.DataFrame(pc))\n",
    "        sky.append(cloud)\n",
    "\n",
    "    return sky\n",
    "\n",
    "def convert_to_point_cloud_bad(arr):\n",
    "\n",
    "    arr['stubR'] = get_stub_r(arr['stubType'], arr['stubDetId'], arr['stubLogicLayer'])\n",
    "    \n",
    "    pc = {'x': arr['stubR'], 'y': arr['stubEta'], 'z': arr['stubPhi']} # down the line maybe convert to actual cartesian coordinates\n",
    "    print('THECLOUD', pc)\n",
    "    # Create a PyntCloud object from the DataFrame\n",
    "    cloud = PyntCloud(pd.DataFrame(pc))\n",
    "    return cloud\n",
    "\n",
    "\n",
    "sky = convert_to_point_cloud(branches)\n",
    "\n",
    "#print(sky[0].points)\n",
    "#print(sky[0].points.describe())\n",
    "#sky[0].points.boxplot()\n",
    "#plt.show()\n",
    "#scatter_matrix(sky[0].points, diagonal=\"kde\", figsize=(8,8))\n",
    "#plt.show()\n",
    "\n",
    "# Create a list to store individual graphs\n",
    "graphs = []\n",
    "\n",
    "logicLayersConnectionMap={\n",
    "    #(0,2), (2,4), (0,6), (2,6), (4,6), (6,7), (6,8), (0,7), (0,9), (9,7), (7,8)]\n",
    "    # Put here catalog of names0\n",
    "    0: [2,6,7],\n",
    "    2: [4,6],\n",
    "    4: [6],\n",
    "    6: [7,8],\n",
    "    7: [8],\n",
    "    8: [],\n",
    "    9: [7],\n",
    "}\n",
    "\n",
    "def getEdgesFromLogicLayer(logicLayer):\n",
    "    return (logicLayersConnectionMap[logicLayer] if logicLayer<10 else [])\n",
    "\n",
    "\n",
    "def convert_to_graphs(arr):\n",
    "\n",
    "    arr['stubR'] = get_stub_r(arr['stubType'], arr['stubDetId'], arr['stubEta'], arr['stubLogicLayer'])\n",
    "    \n",
    "    keep=['stubR', 'stubEta', 'stubPhi', 'stubProc', 'stubPhiB', 'stubEtaSigma', 'stubQuality', 'stubBx', 'stubDetId', 'stubType', 'stubTiming', 'stubLogicLayer', 'muonPt']\n",
    "\n",
    "    sky=[]\n",
    "    for index, row in arr.iterrows():\n",
    "        # Here now I need to enrich this with the stublogiclayer etc, for the edges\n",
    "        pc = {} #{'x': row['stubR'], 'y': row['stubEta'], 'z': row['stubPhi'], 'stubLogicLayer': row['stubLogicLayer']} # down the line maybe convert to actual cartesian coordinates\n",
    "        for label in keep:\n",
    "            pc[label] = row[label]\n",
    "        # Create a PyntCloud object from the DataFrame\n",
    "        if len(pc['stubR'])==0:\n",
    "            continue\n",
    "        sky.append(pd.DataFrame(pc))\n",
    "\n",
    "    return sky\n",
    "\n",
    "sky = convert_to_graphs(branches)\n",
    "\n",
    "\n",
    "for index, cloud in enumerate(sky):\n",
    "    graph = nx.DiGraph()\n",
    "    nodes = []\n",
    "    keep=['stubR', 'stubEta', 'stubPhi', 'stubProc', 'stubPhiB', 'stubEtaSigma', 'stubQuality', 'stubBx', 'stubDetId', 'stubType', 'stubTiming', 'stubLogicLayer']\n",
    "    edges=[]\n",
    "    for index, row in cloud.iterrows(): # build edges based on stubLayer\n",
    "        #nodes.append((index, {k: row[k] for k in keep}))\n",
    "        nodes.append((index, { 'x': [row[k] for k in keep], 'y' : np.float64(row['muonPt']) }))\n",
    "        dests=getEdgesFromLogicLayer(row['stubLogicLayer'])\n",
    "        for queriedindex, row in cloud.iterrows():\n",
    "            if queriedindex in dests:\n",
    "                edges.append((index,queriedindex))\n",
    "    # Rename back\n",
    "    #cp=cloud.points.rename(columns={\"x\": \"stubR\", \"y\": \"stubEta\", \"z\": \"stubPhi\", \"muonPt\": \"y\"}, errors=\"raise\")\n",
    "    graph.add_nodes_from(nodes) # Must be the transpose, it reads by colum instead of by row\n",
    "    graph.add_edges_from(edges)\n",
    "    graphs.append(graph)\n",
    "#for index, cloud in enumerate(sky):\n",
    "#    graph = nx.DiGraph()\n",
    "#    edges=[]\n",
    "#    for index, row in cloud.points.iterrows(): # build edges based on stubLayer\n",
    "#        dests=getEdgesFromLogicLayer(row['stubLogicLayer'])\n",
    "#        for queriedindex, row in cloud.points.iterrows():\n",
    "#            if queriedindex in dests:\n",
    "#                edges.append((index,queriedindex))\n",
    "#    # Rename back\n",
    "#    cp=cloud.points.rename(columns={\"x\": \"stubR\", \"y\": \"stubEta\", \"z\": \"stubPhi\", \"muonPt\": \"y\"}, errors=\"raise\")\n",
    "#    graph.add_nodes_from(cp.T) # Must be the transpose, it reads by colum instead of by row\n",
    "#    graph.add_edges_from(edges)\n",
    "#    graphs.append(graph)\n",
    "\n",
    "\n",
    "viz=True\n",
    "if viz:\n",
    "    gmax=None\n",
    "    nmax=0\n",
    "    for graph in graphs:\n",
    "        numnodes = graph.number_of_nodes()\n",
    "        if numnodes>nmax:\n",
    "            gmax=copy.deepcopy(graph)\n",
    "            nmax=numnodes\n",
    "    nx.draw(gmax, with_labels=True)\n",
    "    print(gmax.nodes.data(True))\n",
    "    g=from_networkx(gmax)\n",
    "    print(g)\n",
    "    print()\n",
    "    print(g.y)\n",
    "    print(g.edge_index)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert each NetworkX graph to a PyTorch Geometric Data object\n",
    "pg_graphs=[from_networkx(g) for g in graphs]\n",
    "\n",
    "\n",
    "pg=None\n",
    "nmax=0\n",
    "for g in pg_graphs:\n",
    "    numnodes=g.num_nodes\n",
    "    if numnodes>nmax:\n",
    "        pg=copy.deepcopy(g)\n",
    "        nmax=numnodes\n",
    "\n",
    "print('networkx graph')\n",
    "print(pg)\n",
    "print(pg.x)\n",
    "print(pg.y)\n",
    "print(pg.edge_index)\n",
    "print('-----------------')\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {pg.num_nodes}') #Number of nodes in the graph\n",
    "print(f'Number of edges: {pg.num_edges}') #Number of edges in the graph\n",
    "print(f'Average node degree: {pg.num_edges / pg.num_nodes:.2f}') # Average number of nodes in the graph\n",
    "print(f'Contains isolated nodes: {pg.has_isolated_nodes()}') #Does the graph contains nodes that are not connected\n",
    "print(f'Contains self-loops: {pg.has_self_loops()}') #Does the graph contains nodes that are linked to themselves\n",
    "print(f'Is undirected: {pg.is_undirected()}') #Is the graph an undirected graph\n",
    "nx.draw(to_networkx(pg), with_labels=True)\n",
    "\n",
    "\n",
    "\n",
    "#data_list = []\n",
    "#for graph in pg_graphs:\n",
    "#    print('pajoerlia', type(graph))\n",
    "#    # Extract node features and labels\n",
    "#    #print(graph, list(graph.nodes), list(graph.edges))\n",
    "#    #print(\"FEATURES\", graph.nodes(data=True))\n",
    "#    #print(\"LABELS\", graph.nodes(data='muonPt'))\n",
    "#    #node_features = graph.nodes(data=True)\n",
    "#    #node_labels = graph.nodes(data='muonPt')\n",
    "#    keep=['stubR', 'stubEta', 'stubPhi', 'stubProc', 'stubPhiB', 'stubEtaSigma', 'stubQuality', 'stubBx', 'stubDetId', 'stubType', 'stubTiming', 'stubLogicLayer', 'muonPt']\n",
    "#    print('mehhhhhh', graph.nodes(data='stubProc'))\n",
    "#    # Convert to PyTorch tensors\n",
    "#    x = torch.tensor([graph.nodes(data=feat) for feat in node_features], dtype=torch.float32)\n",
    "#    y = torch.tensor([graph.nodes(data=label) for label in node_labels], dtype=torch.float32)\n",
    "#\n",
    "#    # Assuming your graph has edges\n",
    "#    edge_index = torch.tensor(list(graph.edges), dtype=torch.long).t().contiguous()\n",
    "#\n",
    "#    # Create a PyTorch Geometric Data object\n",
    "#    data = Data(x=x, edge_index=edge_index, y=y)\n",
    "#\n",
    "#    data_list.append(data)\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Once it's shuffled, we slice the data to split\n",
    "train_dataset = pg_graphs[:int(121/2)]\n",
    "test_dataset = pg_graphs[int(121/2):]\n",
    "\n",
    "#for d in train_dataset:\n",
    "#    d.to(torch.device(\"mps\"))\n",
    "#for d in test_dataset:\n",
    "#    d.to(torch.device(\"mps\"))\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "# Define a simple GNN model\n",
    "import torch.nn.functional as F\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_channels, num_outputs):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, num_channels)\n",
    "        self.conv2 = GCNConv(num_channels, num_outputs)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = x.double()\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.double()\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model=None\n",
    "dotrain=True  # move this to runtime option\n",
    "if not dotrain:\n",
    "    model = torch.load('models/model.pth')\n",
    "    model.eval()\n",
    "else:\n",
    "    model = GCN(pg_graphs[0].num_node_features,8,1)#.to(torch.device(\"mps\"))\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss=0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #total_loss += float(loss) * data.num_graphs\n",
    "        total_loss += float(loss)\n",
    "\n",
    "    return total_loss/len(train_loader.dataset)\n",
    "\n",
    "def test():\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        total_loss = 0\n",
    "        for data in test_loader:\n",
    "            out = model(data)\n",
    "            loss = criterion(out, data.y.unsqueeze(1))\n",
    "            #total_loss += float(loss) * data.num_graphs\n",
    "            total_loss += float(loss)\n",
    "        return total_loss/len(test_loader.dataset)\n",
    "\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "\n",
    "\n",
    "if dotrain:\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train()\n",
    "        test_loss = test()\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        print(f'Epoch: {epoch:02d}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}')\n",
    "        torch.save(model, 'models/mode.pth')\n",
    "    epochs = [ x for x in range(num_epochs)]\n",
    "    plt.plot(epochs, train_losses, label=\"Train loss\")\n",
    "    plt.plot(epochs, test_losses, label=\"Test loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE loss (rescaled)\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "truth=[]\n",
    "pred=[]\n",
    "for data in train_loader:\n",
    "    out = model(data)\n",
    "    for o, t in zip(out, data.y):\n",
    "        pred.append(o.detach().numpy())\n",
    "        truth.append(t.detach().numpy())\n",
    "\n",
    "plt.scatter(truth, pred)\n",
    "plt.xlabel(\"True muon pT\")\n",
    "plt.ylabel(\"Predicted muon pT\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#nonsensical for the moment\n",
    "plt.hist(pred, label=\"Predicted muon pT\")\n",
    "plt.hist(truth, label=\"True muon pT\")\n",
    "plt.show()\n",
    "quit()\n",
    "\n",
    "# I will remove most of what is below when designing a better structure of the GNN and cleaning up the code, next week (17december onwards),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476eac2b",
   "metadata": {
    "cell_marker": "##########################################################################################################################################################################################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "####################################################################################################################\n",
    "#########################################################################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a22c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "model.eval()\n",
    "pred = model(test_loader[0]).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')\n",
    "print(\"TRAINED\")\n",
    "\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = DataLoader(data_list, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define a simple GNN model\n",
    "class SimpleGNN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_classes):\n",
    "        super(SimpleGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, out_channels)\n",
    "        self.conv2 = GCNConv(out_channels, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = SimpleGNN(in_channels=num_features, out_channels=16, num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(\"TRAINED\")\n",
    "## Iterate through each event and create a graph\n",
    "#for index, row in branches.iterrows():\n",
    "#    # Create a directed graph for each event\n",
    "#    graph = nx.DiGraph()\n",
    "#    \n",
    "#    # Add nodes with attributes\n",
    "#    node_attributes = row.to_dict()\n",
    "#    graph.add_node(index, **node_attributes)\n",
    "#    \n",
    "#    # Add the graph to the list\n",
    "#    graphs.append(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa27d48f",
   "metadata": {
    "cell_marker": "####################################"
   },
   "source": [
    "So, what I do below is basically wrong, in the sense that it runs, but it doesn-t contain the edges structure that would be needed.\n",
    "Therefore, here would go the code that creates the edges structure\n",
    "A sample code (for the tracker) is here>  https://github.com/CMS-GNN-Tracking-Hackathon-2021/interaction-network/blob/main/graph_construction/build_graph.py ,\n",
    "where they define the allowed edges between the various parts of the tracker, in their case\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f610c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assume your graph has node features and labels\n",
    "# You should replace this with your actual node features and labels\n",
    "node_features = torch.randn((num_nodes, num_features))  # Replace with your actual features\n",
    "node_labels = torch.randint(0, num_classes, (num_nodes,))  # Replace with your actual labels\n",
    "\n",
    "# Convert each NetworkX graph to a PyTorch Geometric Data object\n",
    "data_list = []\n",
    "for graph in list_of_networkx_graphs:\n",
    "    # Extract node features and labels\n",
    "    node_features = graph.nodes(data='features')\n",
    "    node_labels = graph.nodes(data='label')\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    x = torch.tensor([node['features'] for node in node_features], dtype=torch.float64)\n",
    "    y = torch.tensor([node['label'] for node in node_labels], dtype=torch.long)\n",
    "\n",
    "    # Assuming your graph has edges\n",
    "    edge_index = torch.tensor(list(graph.edges), dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Create a PyTorch Geometric Data object\n",
    "    data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "    data_list.append(data)\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = DataLoader(data_list, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define a simple GNN model\n",
    "class SimpleGNN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_classes):\n",
    "        super(SimpleGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, out_channels)\n",
    "        self.conv2 = GCNConv(out_channels, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = SimpleGNN(in_channels=num_features, out_channels=16, num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "dataset = OMTFDataset(\"data/\", \"point_clouds.hd5\")\n",
    "\n",
    "\n",
    "inspectDataset=False\n",
    "\n",
    "if inspectDataset:\n",
    "    print(dataset)\n",
    "    for image, label in tfds.as_numpy(dataset):\n",
    "        print(type(image), type(label), label)\n",
    "\n",
    "print('Explore dataset')\n",
    "print(f'Number of graphs in the dataset: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "#print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "\n",
    "#from torch_geometric.transforms import KNNGraph\n",
    "#import torch_geometric\n",
    "#\n",
    "#dataset.transform = torch_geometric.transforms.Compose([dataset, KNNGraph(k=6)])\n",
    "#\n",
    "##Since we have one graph in the dataset, we will select the graph and explore it's properties\n",
    "#print(dataset.transform)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = dataset[0]\n",
    "print('Graph properties')\n",
    "print('==============================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}') #Number of nodes in the graph\n",
    "print(f'Number of edges: {data.num_edges}') #Number of edges in the graph\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}') # Average number of nodes in the graph\n",
    "print(f'Contains isolated nodes: {data.has_isolated_nodes()}') #Does the graph contains nodes that are not connected\n",
    "print(f'Contains self-loops: {data.has_self_loops()}') #Does the graph contains nodes that are linked to themselves\n",
    "print(f'Is undirected: {data.is_undirected()}') #Is the graph an undirected graph\n",
    "\n",
    "\n",
    "\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "visualize_graph(G, color=data.y)\n",
    "\n",
    "\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_features, 4)\n",
    "        self.conv2 = GCNConv(4, 4)\n",
    "        self.conv3 = GCNConv(4, 2)\n",
    "        self.classifier = Linear(2, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = h.tanh()\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = h.tanh()\n",
    "        h = self.conv3(h, edge_index)\n",
    "        h = h.tanh()  # Final GNN embedding space.\n",
    "        \n",
    "        # Apply a final (linear) classifier.\n",
    "        out = self.classifier(h)\n",
    "\n",
    "        return out, h\n",
    "\n",
    "model = GCN()\n",
    "print(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = GCN()\n",
    "criterion = torch.nn.CrossEntropyLoss()  #Initialize the CrossEntropyLoss function.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Initialize the Adam optimizer.\n",
    "\n",
    "def train(data):\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out, h = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    return loss, h\n",
    "\n",
    "for epoch in range(401):\n",
    "    loss, h = train(data)\n",
    "    print(f'Epoch: {epoch}, Loss: {loss}')\n",
    "   \n",
    "\n",
    "\n",
    "visualize_graph(model, color=data.y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# What comes below mostly still doesn't make any sense---we don't want to do supervised learning here.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(branches, branches, test_size=0.33)\n",
    "\n",
    "train_dataset = OMTFDataset(X_train, y_train)\n",
    "test_dataset = OMTFDataset(X_test, y_test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "# A simple neural network, to start with\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(14, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, scheduler):\n",
    "    size = len(dataloader.dataset)\n",
    "    losses=[]\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X[:,:-3]) # Change this\n",
    "        #if (all_equal3(pred.detach().numpy())):\n",
    "        #    print(\\\"All equal!\\\")\n",
    "        loss = loss_fn(pred, y)\n",
    "        losses.append(loss.item())\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    scheduler.step()\n",
    "    return np.mean(losses)\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    losses=[]\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X[:,:-3]) # Change this\n",
    "            loss = loss_fn(pred, y).item()\n",
    "            losses.append(loss)\n",
    "            test_loss += loss\n",
    "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    return np.mean(losses)\n",
    "    #test_loss /= num_batches\n",
    "    #correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\\n\")\n",
    "\n",
    "epochs = 30\n",
    "learningRate = 0.01\n",
    "\n",
    "model = NeuralNetwork()\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss=train_loop(train_dataloader, model, loss_fn, optimizer, scheduler)\n",
    "    test_loss=test_loop(test_dataloader, model, loss_fn)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    print(\"Avg train loss\", train_loss, \", Avg test loss\", test_loss, \"Current learning rate\", scheduler.get_last_lr())\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "plt.plot(train_losses, label=\"Average training loss\")\n",
    "plt.plot(test_losses, label=\"Average test loss\")\n",
    "plt.legend(loc=\"best\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
