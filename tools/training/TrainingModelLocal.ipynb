{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504a19d5-3826-4061-b04e-624faa83d938",
   "metadata": {},
   "source": [
    "# Graph Neural Network Training\n",
    "\n",
    "Creation of graphs were done with \n",
    "```tools/training/GraphCreationModel.py```\n",
    "Files can be found in: \n",
    "```/eos/cms/store/user/folguera/L1TMuon/INTREPID/Graphs_v240725_241015/```\n",
    "in two flavours, with \"all\" connected layers and with up to \"3-neighbour\" layers connections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8a9bb6-0343-4241-8191-af45f93f8f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainModelFromGraph import TrainModelFromGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b72ed9a-7364-4ad5-b85a-00b967c65487",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8085132-9490-4c1f-b21b-3647e0fa22d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainModelFromGraph.__init__() got an unexpected keyword argument 'UsingOnly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m Epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m     12\u001b[0m UsingOnly \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m---> 14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainModelFromGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGraph_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraphDIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOut_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mModelOutDIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatchSize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBatchSize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLearningRate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLearningRate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEpochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEpochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mUsingOnly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsingOnly\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m trainer\u001b[38;5;241m.\u001b[39mactivate_debug()\n",
      "\u001b[1;31mTypeError\u001b[0m: TrainModelFromGraph.__init__() got an unexpected keyword argument 'UsingOnly'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "## check if EOS folder exists otherwise use local folder\n",
    "if os.path.exists(\"/eos/cms/store/user/folguera/L1TMuon/INTREPID/Graphs_v240725_241015/\"):\n",
    "    GraphDIR = \"/eos/cms/store/user/folguera/L1TMuon/INTREPID/Graphs_v240725_241015/\"\n",
    "else:\n",
    "    GraphDIR = \"../../graph_folder/\"\n",
    "\n",
    "ModelOutDIR = \"Bsize64_lr5e-4_NOnormNodes/\"\n",
    "BatchSize = 64\n",
    "LearningRate = 0.0005\n",
    "Epochs = 500\n",
    "UsingOnly = 5\n",
    "\n",
    "trainer = TrainModelFromGraph(Graph_path=GraphDIR, Out_path=ModelOutDIR, BatchSize=BatchSize, LearningRate=LearningRate, Epochs=Epochs, UsingOnly=UsingOnly)\n",
    "trainer.activate_debug()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d889516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using files: ['vix_graph_3_15Oct_onlypt_001.pkl', 'vix_graph_3_15Oct_onlypt_002.pkl', 'vix_graph_3_15Oct_onlypt_003.pkl', 'vix_graph_3_15Oct_onlypt_004.pkl', 'vix_graph_3_15Oct_onlypt_005.pkl', 'vix_graph_3_15Oct_onlypt_006.pkl', 'vix_graph_3_15Oct_onlypt_007.pkl', 'vix_graph_3_15Oct_onlypt_008.pkl', 'vix_graph_3_15Oct_onlypt_009.pkl', 'vix_graph_3_15Oct_onlypt_010.pkl', 'vix_graph_3_15Oct_onlypt_011.pkl', 'vix_graph_3_15Oct_onlypt_012.pkl', 'vix_graph_3_15Oct_onlypt_013.pkl', 'vix_graph_3_15Oct_onlypt_014.pkl', 'vix_graph_3_15Oct_onlypt_015.pkl', 'vix_graph_3_15Oct_onlypt_016.pkl', 'vix_graph_3_15Oct_onlypt_017.pkl', 'vix_graph_3_15Oct_onlypt_018.pkl', 'vix_graph_3_15Oct_onlypt_019.pkl', 'vix_graph_3_15Oct_onlypt_020.pkl', 'vix_graph_3_15Oct_onlypt_021.pkl', 'vix_graph_3_15Oct_onlypt_022.pkl', 'vix_graph_3_15Oct_onlypt_023.pkl', 'vix_graph_3_15Oct_onlypt_024.pkl', 'vix_graph_3_15Oct_onlypt_025.pkl', 'vix_graph_3_15Oct_onlypt_026.pkl', 'vix_graph_3_15Oct_onlypt_027.pkl', 'vix_graph_3_15Oct_onlypt_028.pkl', 'vix_graph_3_15Oct_onlypt_029.pkl', 'vix_graph_3_15Oct_onlypt_030.pkl', 'vix_graph_3_15Oct_onlypt_031.pkl', 'vix_graph_3_15Oct_onlypt_032.pkl', 'vix_graph_3_15Oct_onlypt_033.pkl', 'vix_graph_3_15Oct_onlypt_034.pkl', 'vix_graph_3_15Oct_onlypt_035.pkl', 'vix_graph_3_15Oct_onlypt_036.pkl', 'vix_graph_3_15Oct_onlypt_037.pkl', 'vix_graph_3_15Oct_onlypt_038.pkl', 'vix_graph_3_15Oct_onlypt_039.pkl', 'vix_graph_3_15Oct_onlypt_040.pkl', 'vix_graph_3_15Oct_onlypt_041.pkl', 'vix_graph_3_15Oct_onlypt_042.pkl', 'vix_graph_3_15Oct_onlypt_043.pkl', 'vix_graph_3_15Oct_onlypt_044.pkl', 'vix_graph_3_15Oct_onlypt_045.pkl', 'vix_graph_3_15Oct_onlypt_046.pkl', 'vix_graph_3_15Oct_onlypt_047.pkl', 'vix_graph_3_15Oct_onlypt_048.pkl', 'vix_graph_3_15Oct_onlypt_049.pkl', 'vix_graph_3_15Oct_onlypt_050.pkl', 'vix_graph_3_15Oct_onlypt_051.pkl', 'vix_graph_3_15Oct_onlypt_052.pkl', 'vix_graph_3_15Oct_onlypt_054.pkl', 'vix_graph_3_15Oct_onlypt_055.pkl', 'vix_graph_3_15Oct_onlypt_056.pkl', 'vix_graph_3_15Oct_onlypt_057.pkl', 'vix_graph_3_15Oct_onlypt_058.pkl', 'vix_graph_3_15Oct_onlypt_059.pkl', 'vix_graph_3_15Oct_onlypt_060.pkl', 'vix_graph_3_15Oct_onlypt_061.pkl', 'vix_graph_3_15Oct_onlypt_062.pkl', 'vix_graph_3_15Oct_onlypt_063.pkl', 'vix_graph_3_15Oct_onlypt_064.pkl', 'vix_graph_3_15Oct_onlypt_065.pkl', 'vix_graph_3_15Oct_onlypt_066.pkl', 'vix_graph_3_15Oct_onlypt_067.pkl', 'vix_graph_3_15Oct_onlypt_068.pkl', 'vix_graph_3_15Oct_onlypt_069.pkl', 'vix_graph_3_15Oct_onlypt_070.pkl', 'vix_graph_3_15Oct_onlypt_071.pkl', 'vix_graph_3_15Oct_onlypt_072.pkl', 'vix_graph_3_15Oct_onlypt_073.pkl', 'vix_graph_3_15Oct_onlypt_074.pkl', 'vix_graph_3_15Oct_onlypt_075.pkl', 'vix_graph_3_15Oct_onlypt_076.pkl', 'vix_graph_3_15Oct_onlypt_077.pkl', 'vix_graph_3_15Oct_onlypt_078.pkl', 'vix_graph_3_15Oct_onlypt_079.pkl', 'vix_graph_3_15Oct_onlypt_080.pkl', 'vix_graph_3_15Oct_onlypt_081.pkl', 'vix_graph_3_15Oct_onlypt_082.pkl', 'vix_graph_3_15Oct_onlypt_083.pkl', 'vix_graph_3_15Oct_onlypt_084.pkl', 'vix_graph_3_15Oct_onlypt_085.pkl', 'vix_graph_3_15Oct_onlypt_086.pkl', 'vix_graph_3_15Oct_onlypt_087.pkl', 'vix_graph_3_15Oct_onlypt_088.pkl', 'vix_graph_3_15Oct_onlypt_089.pkl', 'vix_graph_3_15Oct_onlypt_090.pkl', 'vix_graph_3_15Oct_onlypt_091.pkl', 'vix_graph_3_15Oct_onlypt_092.pkl', 'vix_graph_3_15Oct_onlypt_093.pkl', 'vix_graph_3_15Oct_onlypt_094.pkl', 'vix_graph_3_15Oct_onlypt_095.pkl', 'vix_graph_3_15Oct_onlypt_096.pkl', 'vix_graph_3_15Oct_onlypt_097.pkl', 'vix_graph_3_15Oct_onlypt_098.pkl', 'vix_graph_3_15Oct_onlypt_099.pkl', 'vix_graph_3_15Oct_onlypt_100.pkl', 'vix_graph_3_15Oct_onlypt_101.pkl', 'vix_graph_3_15Oct_onlypt_102.pkl', 'vix_graph_3_15Oct_onlypt_103.pkl', 'vix_graph_3_15Oct_onlypt_104.pkl', 'vix_graph_3_15Oct_onlypt_105.pkl', 'vix_graph_3_15Oct_onlypt_106.pkl', 'vix_graph_3_15Oct_onlypt_107.pkl', 'vix_graph_3_15Oct_onlypt_108.pkl', 'vix_graph_3_15Oct_onlypt_109.pkl', 'vix_graph_3_15Oct_onlypt_110.pkl', 'vix_graph_3_15Oct_onlypt_111.pkl', 'vix_graph_3_15Oct_onlypt_112.pkl', 'vix_graph_3_15Oct_onlypt_113.pkl', 'vix_graph_3_15Oct_onlypt_114.pkl', 'vix_graph_3_15Oct_onlypt_115.pkl', 'vix_graph_3_15Oct_onlypt_116.pkl', 'vix_graph_3_15Oct_onlypt_117.pkl', 'vix_graph_3_15Oct_onlypt_118.pkl', 'vix_graph_3_15Oct_onlypt_119.pkl', 'vix_graph_3_15Oct_onlypt_120.pkl', 'vix_graph_3_15Oct_onlypt_121.pkl', 'vix_graph_3_15Oct_onlypt_122.pkl', 'vix_graph_3_15Oct_onlypt_123.pkl', 'vix_graph_3_15Oct_onlypt_124.pkl', 'vix_graph_3_15Oct_onlypt_125.pkl', 'vix_graph_3_15Oct_onlypt_126.pkl', 'vix_graph_3_15Oct_onlypt_127.pkl', 'vix_graph_3_15Oct_onlypt_128.pkl', 'vix_graph_3_15Oct_onlypt_129.pkl', 'vix_graph_3_15Oct_onlypt_130.pkl', 'vix_graph_3_15Oct_onlypt_131.pkl', 'vix_graph_3_15Oct_onlypt_132.pkl', 'vix_graph_3_15Oct_onlypt_133.pkl', 'vix_graph_3_15Oct_onlypt_134.pkl', 'vix_graph_3_15Oct_onlypt_135.pkl', 'vix_graph_3_15Oct_onlypt_136.pkl', 'vix_graph_3_15Oct_onlypt_137.pkl', 'vix_graph_3_15Oct_onlypt_138.pkl', 'vix_graph_3_15Oct_onlypt_139.pkl', 'vix_graph_3_15Oct_onlypt_140.pkl', 'vix_graph_3_15Oct_onlypt_141.pkl', 'vix_graph_3_15Oct_onlypt_142.pkl', 'vix_graph_3_15Oct_onlypt_143.pkl', 'vix_graph_3_15Oct_onlypt_144.pkl', 'vix_graph_3_15Oct_onlypt_145.pkl', 'vix_graph_3_15Oct_onlypt_146.pkl', 'vix_graph_3_15Oct_onlypt_147.pkl', 'vix_graph_3_15Oct_onlypt_148.pkl', 'vix_graph_3_15Oct_onlypt_149.pkl', 'vix_graph_3_15Oct_onlypt_150.pkl', 'vix_graph_3_15Oct_onlypt_151.pkl', 'vix_graph_3_15Oct_onlypt_152.pkl', 'vix_graph_3_15Oct_onlypt_153.pkl', 'vix_graph_3_15Oct_onlypt_154.pkl', 'vix_graph_3_15Oct_onlypt_155.pkl', 'vix_graph_3_15Oct_onlypt_156.pkl', 'vix_graph_3_15Oct_onlypt_157.pkl', 'vix_graph_3_15Oct_onlypt_158.pkl', 'vix_graph_3_15Oct_onlypt_159.pkl', 'vix_graph_3_15Oct_onlypt_160.pkl', 'vix_graph_3_15Oct_onlypt_161.pkl', 'vix_graph_3_15Oct_onlypt_162.pkl', 'vix_graph_3_15Oct_onlypt_163.pkl', 'vix_graph_3_15Oct_onlypt_164.pkl', 'vix_graph_3_15Oct_onlypt_165.pkl', 'vix_graph_3_15Oct_onlypt_166.pkl', 'vix_graph_3_15Oct_onlypt_167.pkl', 'vix_graph_3_15Oct_onlypt_168.pkl', 'vix_graph_3_15Oct_onlypt_169.pkl', 'vix_graph_3_15Oct_onlypt_170.pkl', 'vix_graph_3_15Oct_onlypt_171.pkl', 'vix_graph_3_15Oct_onlypt_172.pkl', 'vix_graph_3_15Oct_onlypt_173.pkl', 'vix_graph_3_15Oct_onlypt_174.pkl', 'vix_graph_3_15Oct_onlypt_175.pkl', 'vix_graph_3_15Oct_onlypt_176.pkl', 'vix_graph_3_15Oct_onlypt_177.pkl', 'vix_graph_3_15Oct_onlypt_178.pkl', 'vix_graph_3_15Oct_onlypt_179.pkl', 'vix_graph_3_15Oct_onlypt_180.pkl', 'vix_graph_3_15Oct_onlypt_181.pkl', 'vix_graph_3_15Oct_onlypt_182.pkl', 'vix_graph_3_15Oct_onlypt_183.pkl', 'vix_graph_3_15Oct_onlypt_184.pkl', 'vix_graph_3_15Oct_onlypt_185.pkl', 'vix_graph_3_15Oct_onlypt_186.pkl', 'vix_graph_3_15Oct_onlypt_187.pkl', 'vix_graph_3_15Oct_onlypt_188.pkl', 'vix_graph_3_15Oct_onlypt_190.pkl', 'vix_graph_3_15Oct_onlypt_191.pkl', 'vix_graph_3_15Oct_onlypt_192.pkl', 'vix_graph_3_15Oct_onlypt_193.pkl', 'vix_graph_3_15Oct_onlypt_194.pkl', 'vix_graph_3_15Oct_onlypt_195.pkl', 'vix_graph_3_15Oct_onlypt_196.pkl', 'vix_graph_3_15Oct_onlypt_197.pkl', 'vix_graph_3_15Oct_onlypt_198.pkl', 'vix_graph_3_15Oct_onlypt_199.pkl', 'vix_graph_3_15Oct_onlypt_200.pkl']\n",
      "Total Graphs: 335111\n",
      "Normalizing the graphs\n",
      "====================================\n",
      "Example of data (before normalization):\n",
      "tensor([[1.0331e+00, 9.8300e+02, 1.0060e+03, 6.0000e+00, 9.0000e+00],\n",
      "        [1.0114e+00, 9.7700e+02, 1.0767e+03, 1.5000e+01, 5.0000e+00],\n",
      "        [1.0440e+00, 9.9100e+02, 1.1959e+03, 7.0000e+00, 9.0000e+00],\n",
      "        [1.1092e+00, 9.5400e+02, 4.1368e+02, 1.0000e+01, 5.0000e+00]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[0, 0, 0, 1, 1, 2, 2, 3],\n",
      "        [1, 2, 3, 0, 2, 0, 1, 0]])\n",
      "tensor([ -6,   8, -29,  -6, -14,   8, -14, -29])\n",
      "tensor([-0.0217,  0.0109,  0.0761, -0.0217, -0.0326,  0.0109, -0.0326,  0.0761])\n",
      "tensor([-10.3202])\n",
      "====================================\n",
      "Example of data (after normalization):\n",
      "tensor([[1.0331e+00, 9.8300e+02, 1.0060e+03, 6.0000e+00, 9.0000e+00],\n",
      "        [1.0114e+00, 9.7700e+02, 1.0767e+03, 1.5000e+01, 5.0000e+00],\n",
      "        [1.0440e+00, 9.9100e+02, 1.1959e+03, 7.0000e+00, 9.0000e+00],\n",
      "        [1.1092e+00, 9.5400e+02, 4.1368e+02, 1.0000e+01, 5.0000e+00]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[0, 0, 0, 1, 1, 2, 2, 3],\n",
      "        [1, 2, 3, 0, 2, 0, 1, 0]])\n",
      "tensor([ -6,   8, -29,  -6, -14,   8, -14, -29])\n",
      "tensor([-0.0217,  0.0109,  0.0761, -0.0217, -0.0326,  0.0109, -0.0326,  0.0761])\n",
      "tensor([-10.3202])\n",
      "Training events: 234432\n",
      "====================================\n",
      "Example of data:\n",
      "tensor([[1.0331e+00, 9.8300e+02, 1.0060e+03, 6.0000e+00, 9.0000e+00],\n",
      "        [1.0114e+00, 9.7700e+02, 1.0767e+03, 1.5000e+01, 5.0000e+00],\n",
      "        [1.0440e+00, 9.9100e+02, 1.1959e+03, 7.0000e+00, 9.0000e+00],\n",
      "        [1.1092e+00, 9.5400e+02, 4.1368e+02, 1.0000e+01, 5.0000e+00]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[0, 0, 0, 1, 1, 2, 2, 3],\n",
      "        [1, 2, 3, 0, 2, 0, 1, 0]])\n",
      "tensor([ -6,   8, -29,  -6, -14,   8, -14, -29])\n",
      "tensor([-0.0217,  0.0109,  0.0761, -0.0217, -0.0326,  0.0109, -0.0326,  0.0761])\n",
      "tensor([-10.3202])\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "trainer.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b71ac25c-5006-496c-9545-d22231c5cf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model initialized\n",
      "GATRegressor(\n",
      "  (conv1): GATConv(5, 64, heads=1)\n",
      "  (conv2): GATConv(64, 64, heads=1)\n",
      "  (fc1): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Try to compile model...\n",
      "Model compiled\n"
     ]
    }
   ],
   "source": [
    "trainer.initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4312d8fa-851d-409a-953e-460dcc25b445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current CUDA device: 0\n",
      "CUDA device name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "print(\"Current CUDA device:\", torch.cuda.current_device())\n",
    "print(\"CUDA device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f49ce18-40cc-4df3-9737-d2559e40aaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Epoch: 01, Train loss: 10.1278, Test loss: 8.8727\n"
     ]
    }
   ],
   "source": [
    "trainer.Debug=False\n",
    "trainer.loop_over_epochs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc51e2b-1b30-4624-9310-e5ecbd85deda",
   "metadata": {},
   "source": [
    "# Other (older) tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a2c49-4267-4b6d-85c1-bf02a79be9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BatchSize=64\n",
    "\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "# Normalización de los datos\n",
    "class NormalizeEdgeFeatures(object):\n",
    "    def __call__(self, data):        \n",
    "        # Normalizar características de los bordes\n",
    "        edge_attr = torch.stack([data.deltaPhi, data.deltaEta], dim=1)\n",
    "        edge_attr = (edge_attr - edge_attr.mean(dim=0)) / edge_attr.std(dim=0)\n",
    "        data.deltaPhi, data.deltaEta = edge_attr[:, 0], edge_attr[:, 1]\n",
    "        \n",
    "        return data\n",
    "\n",
    "class NormalizeNodeAndEdgeFeatures(BaseTransform):\n",
    "    def __call__(self, data):\n",
    "        # Normalizar características de los nodos\n",
    "        data.x = (data.x - data.x.mean(dim=0)) / data.x.std(dim=0)\n",
    "        \n",
    "        # Normalizar características de los bordes\n",
    "        edge_attr = torch.stack([data.deltaPhi, data.deltaEta], dim=1)\n",
    "        edge_attr = (edge_attr - edge_attr.mean(dim=0)) / edge_attr.std(dim=0)\n",
    "        data.deltaPhi, data.deltaEta = edge_attr[:, 0], edge_attr[:, 1]\n",
    "        \n",
    "        return data\n",
    "\n",
    "\n",
    "# Aplicar transformaciones de normalización\n",
    "#transform = T.Compose([T.RemoveIsolatedNodes(),T.NormalizeFeatures()]) #NormalizeEdgeFeatures()])\n",
    "transform = T.Compose([T.RemoveIsolatedNodes(),NormalizeNodeAndEdgeFeatures()]) \n",
    "\n",
    "Graphs_for_training = sum(Allgraphs, [])\n",
    "Graphs_for_training_reduced = Graphs_for_training\n",
    "Graphs_for_training_filtered = [g for g in Graphs_for_training_reduced if g.edge_index.size(1) > 0]  # remove empty graphs\n",
    "\n",
    "# remove extra dimenson in y\n",
    "print(f\"Total Graphs: {len(Graphs_for_training)}\")\n",
    "for i in range(0, len(Graphs_for_training_filtered)):\n",
    "    Graphs_for_training_filtered[i].y = Graphs_for_training_filtered[i].y.mean(dim=0)\n",
    "\n",
    "print(\"====================================\")\n",
    "print(\"Example of data (before normalization):\")\n",
    "print(Graphs_for_training_filtered[0].x)\n",
    "print(Graphs_for_training_filtered[0].edge_index)\n",
    "print(Graphs_for_training_filtered[0].edge_attr)\n",
    "print(Graphs_for_training_filtered[0].deltaPhi)\n",
    "print(Graphs_for_training_filtered[0].deltaEta)\n",
    "print(Graphs_for_training_filtered[0].y)\n",
    "\n",
    "# Normalize the data...\n",
    "#Graphs_for_training_filtered = normalize_graphs(Graphs_for_training_filtered)\n",
    "\n",
    "Graphs_for_training_filtered = [transform(g) for g in Graphs_for_training_filtered]\n",
    "\n",
    "print(f\"Total Graphs after filtering: {len(Graphs_for_training_filtered)}\")\n",
    "\n",
    "# Train and test split:\n",
    "events = len(Graphs_for_training_filtered)\n",
    "ntrain = int((events * 0.7) / BatchSize) * BatchSize  # to have full batches\n",
    "print(f\"Training events: {ntrain}\")\n",
    "train_dataset = Graphs_for_training_filtered[:ntrain]\n",
    "test_dataset = Graphs_for_training_filtered[ntrain:ntrain * 2]\n",
    "\n",
    "print(\"====================================\")\n",
    "print(\"Example of data (after normalization):\")\n",
    "print(train_dataset[0].x)\n",
    "print(train_dataset[0].edge_index)\n",
    "print(train_dataset[0].edge_attr)\n",
    "print(train_dataset[0].deltaPhi)\n",
    "print(train_dataset[0].deltaEta)\n",
    "print(train_dataset[0].y)\n",
    "print(\"====================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e130fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_loader = DataLoader(train_dataset, batch_size=BatchSize, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad19f4b-ebe3-4cb3-8fe7-71542f3ede76",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa398616-e2a3-490e-8502-2afb1971c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import GATRegressor \n",
    "\n",
    "num_node_features = 5\n",
    "hidden_dim = BatchSize\n",
    "output_dim = 1\n",
    "LearningRate=0.0005\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "model = GATRegressor(num_node_features, hidden_dim, output_dim).to(device)\n",
    "torch.compile(model, dynamic=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LearningRate, weight_decay=0.75)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    self.model = torch.nn.DataParallel(self.model)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "print(\"Model initialized\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f259b03-5be6-46fc-abf0-0105c533c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "#\n",
    "#path = \"/eos/cms/store/user/folguera/L1TMuon/INTREPID/Model_v240725_241022/\"\n",
    "path = \"../model_folder_v2/\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87282a0d-096c-458d-b2ec-baa9102fafa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)  # Mueve los datos al dispositivo\n",
    "        data.y = data.y.float()  # Asegurarse de que los datos sean float32\n",
    "        data.x = data.x.float()  # Asegurarse de que los datos sean float32\n",
    "        out = model(data)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(out, data.y.view(out.size()))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss)\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test():\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            data.x = data.x.float()  # Asegurarse de que los datos sean float32\n",
    "            data.y = data.y.float()  # Asegurarse de que los datos sean float32\n",
    "            out = model(data)\n",
    "            loss = loss_fn(out, data.y.view(out.size()))\n",
    "            total_loss += float(loss)\n",
    "    return total_loss / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "print(\"Start training...\")\n",
    "for epoch in range(100):\n",
    "    train_loss = train()\n",
    "    test_loss = test()\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    if epoch == 0:\n",
    "        print(f'Epoch: {epoch + 1:02d}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}')\n",
    "        torch.save(test_loss, f\"{path}/testloss_{epoch + 1}.pt\")\n",
    "        torch.save(train_loss, f\"{path}/trainloss_{epoch + 1}.pt\")\n",
    "    elif (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch: {epoch + 1:02d}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}')\n",
    "        torch.save(model, f\"{path}/model_{epoch + 1}.pth\")\n",
    "        torch.save(test_loss, f\"{path}/testloss_{epoch + 1}.pt\")\n",
    "        torch.save(train_loss, f\"{path}/trainloss_{epoch + 1}.pt\")\n",
    "\n",
    "        plt.plot(train_losses, \"b\", label=\"Train loss\")\n",
    "        plt.plot(test_losses, \"k\", label=\"Test loss\")\n",
    "        plt.yscale('log')\n",
    "        plt.savefig(f\"{path}/loss_plot.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d0889f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Plotting and checking\n",
    "Now we need to plot everything and check the predicted momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561f26bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotRegression:\n",
    "    def __init__(self, model, test_loader, batch_size):\n",
    "        self.model = model\n",
    "        self.test_loader = test_loader\n",
    "        self.batch_size = batch_size\n",
    "        self.pt_pred_arr = []\n",
    "        self.pt_truth_arr = []\n",
    "\n",
    "    def evaluate(self):\n",
    "        with torch.no_grad():\n",
    "            for data in self.test_loader:\n",
    "                out = self.model(data)\n",
    "                for item in range(0, out.size(0)):\n",
    "                    vector_pred = out[item]\n",
    "                    vector_real = data[item].y\n",
    "                    self.pt_pred_arr.append(vector_pred.item())\n",
    "                    self.pt_truth_arr.append(vector_real.item())\n",
    "\n",
    "    def plot_regression(self, output_dir):\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        plt.clf()\n",
    "        print(f\"Plotting regression in {output_dir}\")\n",
    "        plt.hist(self.pt_truth_arr, bins=100, color='skyblue', alpha=0.5, label=\"truth\")\n",
    "        plt.hist(self.pt_pred_arr, bins=100, color='g', alpha=0.5, label=\"prediction\")\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(output_dir, \"pt_regression.png\"))\n",
    "        plt.clf()\n",
    "\n",
    "        print(f\"Plotting scatter in {output_dir}\")\n",
    "        plt.plot(self.pt_truth_arr, self.pt_pred_arr, 'o')\n",
    "        plt.xlabel(\"Truth\")\n",
    "        plt.ylabel(\"Prediction\")\n",
    "        plt.savefig(os.path.join(output_dir, \"pt_regression_scatter.png\"))\n",
    "        plt.clf()\n",
    "\n",
    "        print(f\"Plotting difference in {output_dir}\")\n",
    "        # plot difference between truth and prediction\n",
    "        diff = [x - y for x, y in zip(self.pt_truth_arr, self.pt_pred_arr)]\n",
    "        plt.hist(diff, bins=100, color='r', alpha=0.5, label=\"difference\")\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(output_dir, \"pt_regression_diff.png\"))\n",
    "        plt.clf()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"../model_folder/model_100.pth\")\n",
    "            \n",
    "evaluator = PlotRegression(model, test_loader, batch_size=BatchSize)\n",
    "evaluator.evaluate()\n",
    "evaluator.plot_regression(output_dir=\"../model_folder/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv_win",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
